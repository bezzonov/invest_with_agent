stock_info = {
    'AFLT': ["""
«Аэрофлот» — крупнейшая авиакомпания России и СНГ с самой большой маршрутной сетью. Выполняет внутренние и международные пассажирские и грузовые перевозки. Компания активно обновляет флот, внедряет цифровые сервисы и стремится к устойчивому развитию в авиационной отрасли.
"""],
    'AKRN': ["""
ПАО «Акрон» — ведущий российский производитель минеральных удобрений, специализируется на выпуске азотных и комплексных удобрений. Компания занимает значительную долю на российском и мировом рынках, активно развивает экспорт и внедряет экологичные технологии производства.
"""],
    'ALRS': ["""
ПАО «Алроса» — крупнейшая в мире компания по добыче алмазов, контролирующая около 30% мировой добычи. Основные месторождения расположены в Якутии и Архангельской области. Компания ведёт масштабные проекты по разведке и развитию новых месторождений, а также занимается огранкой и продажей драгоценных камней.
"""],
    'BANE': ["""
ПАО «Банк Санкт-Петербург» — крупный региональный банк с фокусом на Северо-Запад России. Предоставляет полный спектр банковских услуг для частных и корпоративных клиентов, активно внедряет цифровые технологии и расширяет присутствие на рынке.
"""],
    'BSPB': ["""
ПАО «Банк Санкт-Петербург» — региональный банк, специализирующийся на розничном и корпоративном банкинге, с развитой сетью отделений и цифровых сервисов. Один из лидеров в Северо-Западном регионе по объёму активов и качеству обслуживания.
"""],
    'CBOM': ["""
«Московский кредитный банк» (МКБ) — один из крупнейших частных банков России, оказывающий финансовые услуги физическим и юридическим лицам, включая кредитование, инвестиции и расчётно-кассовое обслуживание. """],
    'CHMF': ["""
ПАО «Северсталь» — один из крупнейших производителей стали в России и мире. Компания выпускает широкий ассортимент стальной продукции, включая листовой прокат, трубы и строительные материалы. Активно инвестирует в модернизацию производства и экологические инициативы.
"""],
    'ENPG': ["""
ПАО «Энел Россия» — дочерняя компания международной энергетической группы Enel, занимается производством и распределением электроэнергии в России. Ведёт проекты по развитию возобновляемых источников энергии и внедрению современных технологий.
"""],
    'FEES': ["""
ПАО «Федеральная сетевая компания Единой энергетической системы» (ФСК ЕЭС) — оператор магистральных электрических сетей России. Обеспечивает передачу электроэнергии на большие расстояния, поддерживает стабильность и надёжность энергосистемы страны.
"""],
    'FESH': ["""
ПАО «Дальневосточное морское пароходство» (ДВМП), входящее в группу FESCO под управлением госкорпорации «Росатом», — ключевой оператор транспортных услуг на Дальнем Востоке России. Осуществляет контейнерные перевозки морским, железнодорожным и автомобильным транспортом, связывая Россию с Азией и СНГ.
"""],
    'FLOT': ["""
ПАО «Совкомфлот» — крупнейшая российская судоходная компания, специализирующаяся на транспортировке нефти, газа и сухих грузов морским путём. Компания активно расширяет флот и внедряет экологические стандарты, укрепляя позиции на мировом рынке.
"""],
    'GAZP': ["""
ПАО «Газпром» — крупнейшая российская и одна из ведущих мировых энергетических компаний с государственным контролем. Основные направления: разведка, добыча, транспортировка и продажа природного газа и нефти. Владеет крупнейшей в мире газотранспортной системой протяжённостью более 180 тыс. км.
"""],
    'GCHE': ["""
ПАО «Группа Черкизово» — ведущий российский производитель мясной продукции, специализируется на птице, свинине и мясных деликатесах. Компания активно расширяет производство и экспорт, внедряет современные технологии переработки.
"""],
    'GMKN': ["""
ПАО «ГМК Норильский Никель» — мировой лидер по производству никеля и палладия, а также крупный производитель платины и меди. Компания ведёт добычу и переработку в Арктическом регионе, внедряет экологичные технологии и социальные программы.
"""],
    'HEAD': ["""
HeadHunter — крупнейшая российская онлайн-платформа для поиска работы и подбора персонала. Предлагает обширную базу вакансий и резюме, а также инструменты для эффективного рекрутинга. В 2024–2025 годах активно развивает ИИ-сервисы и расширяет географию.
"""],
    'HYDR': ["""
ПАО «РусГидро» — крупнейшая российская компания в сфере гидроэнергетики. Управляет большим парком гидроэлектростанций, развивает возобновляемые источники энергии и проекты по модернизации энергосистемы.
"""],
    'IRAO': ["""
ПАО «Интер РАО» — крупный энергетический холдинг, занимающийся производством и поставкой электроэнергии и тепла в России и за рубежом. Активно расширяет экспортные мощности и диверсифицирует энергетический портфель.
"""],
    'IRKT': ["""
ПАО «Иркут» — российский производитель авиационной техники и двигателей, поставщик компонентов для гражданской и военной авиации. Компания участвует в крупных проектах импортозамещения и модернизации парка воздушных судов.
"""],
    'KZOS': ["""
ПАО «Курганмашзавод» — ведущий российский производитель бронетехники и военной техники. Обеспечивает российскую армию современными бронемашинами и спецтехникой, ведёт экспортные поставки.
"""],
    'LKOH': ["""
«Лукойл» — одна из крупнейших вертикально интегрированных нефтегазовых компаний мира. Производит более 2% мировой нефти, владеет крупными проектами в России, Средней Азии и на Ближнем Востоке. Управляет нефтеперерабатывающими заводами в России и Европе, активно развивает экспорт.
"""],
    'MAGN': ["""
ПАО «Магнит» — крупнейшая российская сеть розничных магазинов с широкой географией. Предлагает продукты питания и товары повседневного спроса, активно развивает цифровые сервисы и программы лояльности.
"""],
    'MGNT': ["""
Российская сеть розничных магазинов, основанная в 1994 году. Один из лидеров продуктового ритейла с развитой логистической инфраструктурой и инновационными решениями в торговле.
"""],
    'MOEX': ["""
Московская биржа — ведущая многофункциональная биржевая площадка России, обеспечивающая торги акциями, облигациями, валютой, деривативами и товарами. В 2024 году биржа показала устойчивый рост оборотов и внедрила новые цифровые продукты.
"""],
    'MTSS': ["""
ПАО «МТС» — крупнейший оператор сотовой связи в России и Белоруссии. Предоставляет телекоммуникационные услуги, цифровые и медийные сервисы. В 2024 году обслуживала более 87 млн абонентов, активно развивает 5G и облачные технологии.
"""],
    'NKNC': ["""
ПАО «Новокузнецкий металлургический комбинат» — крупный российский производитель стали, входящий в состав Evraz Group. Производит широкий ассортимент металлопродукции для строительной и машиностроительной отраслей.
"""],
    'NLMK': ["""
Группа НЛМК — крупнейший производитель стали в России и один из ведущих в мире. Использует вертикально интегрированную модель от добычи сырья до выпуска готовой продукции. Имеет производственные площадки в России, Европе, США и Индии.
"""],
    'NMTP': ["""
ПАО «Новороссийский морской торговый порт» — крупнейший портовый оператор России, управляющий портами Новороссийска, Приморска и Балтийска. Один из лидеров по грузообороту в Европе, активно развивает инфраструктуру и сервисы.
"""],
    'NVTK': ["""
ПАО «НОВАТЭК» — крупнейший независимый производитель природного газа в России. Развивает производство сжиженного природного газа (СПГ) и экспортирует его на мировые рынки. Активно инвестирует в новые месторождения и устойчивое развитие.
"""],
    'PHOR': ["""
ПАО «ФосАгро» — один из крупнейших мировых производителей фосфорных удобрений и сырья для химической промышленности. Компания экспортирует продукцию в более чем 50 стран, внедряет экологичные технологии.
"""],
    'PIKK': ["""
ПАО «ПИК» — крупнейший российский девелопер жилой недвижимости. Специализируется на массовом строительстве многоквартирных домов, комплексном освоении территорий и внедрении современных архитектурных решений.
"""],
    'PLZL': ["""
ПАО «Полюс» — ведущая золотодобывающая компания России и одна из крупнейших в мире. Управляет крупными месторождениями золота, внедряет современные методы добычи и устойчивого развития.
"""],
    'POSI': ["""
ПАО «Промсвязьбанк» — крупный российский банк с фокусом на корпоративных клиентах и гособоронзаказе. Активно развивает цифровые банковские сервисы и финансовые продукты для бизнеса.
"""],
    'RASP': ["""
ПАО «Российские железные дороги» (РЖД) — государственная компания, управляющая железнодорожной инфраструктурой России. Обеспечивает перевозки грузов и пассажиров, внедряет цифровизацию и экологичные технологии.
"""],
    'ROSN': ["""
ПАО «Роснефть» — крупнейшая нефтяная компания России и одна из крупнейших в мире. Занимается разведкой, добычей, переработкой нефти и газа, а также производством нефтехимической продукции. В 2025 году рыночная капитализация — около 4,78 трлн рублей.
"""],
    'RTKM': ["""
ПАО «Ростелеком» — крупнейший российский оператор цифровых услуг и связи. Предлагает услуги интернета, телевидения, облачных решений и кибербезопасности для частных и корпоративных клиентов.
"""],
    'RUAL': ["""
ПАО «Русал» — один из мировых лидеров по производству алюминия и алюминиевой продукции. Компания управляет производственными активами в России и за рубежом, активно внедряет экологичные технологии.
"""],
    'SBER': ["""
ПАО «Сбербанк России» — крупнейший универсальный банк России и Восточной Европы с рыночной капитализацией около 6,77 трлн рублей (2025). Обслуживает более 109 млн частных и 3,3 млн корпоративных клиентов, активно развивает цифровые сервисы и экосистему.
"""],
    'SBERP': ["""
Привилегированные акции ПАО «Сбербанк» — дают право на фиксированные дивиденды и преимущество при распределении прибыли. Являются популярным инструментом среди инвесторов, ориентированных на стабильный доход.
"""],
    'SFIN': ["""
ПАО «Система Финанс» — инвестиционная и финансовая компания, управляющая активами в различных секторах экономики, включая телекоммуникации, энергетику и промышленность.
"""],
    'SIBN': ["""
ПАО «СИБУР» — крупная российская нефтехимическая компания, занимающаяся переработкой углеводородов и производством химической продукции. (Возможно, имелась в виду «Сибнефть» — уточните.)"""],
    'SNGS': ["""
ПАО «Сургутнефтегаз» — одна из крупнейших нефтяных компаний России, занимающаяся разведкой, добычей и переработкой нефти и газа. Обладает значительными запасами и развитой инфраструктурой.
"""],
    'SNGSP': ["""
Привилегированные акции ПАО «Сургутнефтегаз» — обеспечивают приоритетное право на дивиденды и стабильный доход для инвесторов.
"""],
    'SVCB': ["""
ПАО «Связь-Банк» — российский коммерческий банк, предоставляющий широкий спектр услуг для корпоративных и частных клиентов, включая кредитование и инвестиционные продукты.
"""],
    'T': ["""
«Т-Банк» (до июня 2024 года — «Тинькофф Банк») — крупнейший российский дистанционный банк, не имеющий физических отделений. Обслуживает миллионы клиентов, предлагает широкий спектр финансовых услуг через цифровые каналы.
"""],
    'TATN': ["""
ПАО «Татнефть» — крупная российская нефтяная компания, занимающаяся разведкой, добычей и переработкой нефти, а также производством нефтепродуктов и химической продукции.
"""],
    'UGLD': ["""
ПАО «Уралкалий» — один из крупнейших в мире производителей калийных удобрений, экспортирует продукцию в десятки стран, активно инвестирует в расширение производственных мощностей.
"""],
    'UNAC': ["""
ПАО «Юнипро» — один из крупнейших производителей электроэнергии в России, владеет и эксплуатирует тепловые электростанции, активно развивает проекты по модернизации и повышению эффективности.
"""],
    'UWGN': ["""
ПАО «Уралвагонзавод» — ведущий российский производитель железнодорожного подвижного состава и бронетехники, поставщик вооружений и техники для армии РФ.
"""],
    'VSMO': ["""
ПАО «Высокие технологии» — российская компания, специализирующаяся на разработке и производстве медицинского оборудования, диагностических систем и технологий для здравоохранения.
"""],
    'VTBR': ["""
ПАО «ВТБ» — один из крупнейших банков России с широкой сетью филиалов. Предлагает полный спектр финансовых услуг для частных и корпоративных клиентов, активно развивает цифровые технологии.
"""],
    'YDEX': ["""
ПАО «Яндекс» — крупнейшая российская IT-компания, лидер в области интернет-поиска, рекламы, такси, доставки и облачных сервисов. В 2024–2025 годах активно развивает искусственный интеллект и экосистемные сервисы. Входит в топ-3 крупнейших российских компаний по капитализации в IT-секторе.
"""],
}




model_info = {
    "A2C (Advantage Actor Critic)" : """
Advantage Actor Critic (A2C) - это алгоритм обучения с подкреплением, который объединяет преимущества двух подходов: обучения на основе политики (policy-based) и обучения на основе значения (value-based). В основе A2C лежит архитектура Actor-Critic, где два компонента работают совместно: актор (Actor), который отвечает за выбор действий, и критик (Critic), который оценивает качество этих действий.

Как работает A2C:

Актор формирует политику - распределение вероятностей действий в каждом состоянии, то есть решает, какое действие выполнить.

Критик оценивает, насколько хорошим было выбранное действие, вычисляя функцию преимущества (Advantage function). Эта функция показывает, насколько действие лучше среднего ожидания для данного состояния.

Функция преимущества вычисляется как разница между оценкой качества конкретного действия и средней ценностью состояния. Если значение преимущества положительно, это означает, что действие было лучше среднего, и актору следует увеличить вероятность его выбора. Если отрицательно - вероятность действия уменьшается.

Преимущества A2C:

Стабилизация обучения: Использование функции преимущества снижает дисперсию градиентов по сравнению с классическими методами Policy Gradient, что ускоряет и стабилизирует процесс обучения.

Синхронность: В отличие от асинхронного A3C, A2C синхронно обновляет параметры, что позволяет эффективнее использовать вычислительные ресурсы, особенно GPU, за счёт больших батчей данных.

Обновление на каждом шаге: Критик обновляется после каждого шага, а не по окончании эпизода, что повышает скорость и качество обучения.

Гибкость: A2C хорошо подходит для задач с непрерывным и дискретным пространством действий, широко применяется в играх и робототехнике.
""",
    "PPO (Proximal Policy Optimization)" : """Proximal Policy Optimization (PPO) - это современный алгоритм обучения с подкреплением, который сочетает в себе эффективность и стабильность при обучении агентов принимать решения в сложных средах. PPO относится к семейству методов оптимизации политики и использует архитектуру актор-критик, где актор отвечает за выбор действий, а критик оценивает качество этих действий.

Основные принципы работы PPO
Оптимизация политики через градиентный спуск: PPO стремится максимизировать ожидаемую награду агента, обновляя параметры политики с помощью градиентного метода.

Контроль изменений политики: Чтобы избежать резких и дестабилизирующих изменений стратегии агента, PPO вводит отношение вероятностей выбора действий новой и старой политиками. Это отношение показывает, насколько сильно обновилась политика.

Механизм clipping: PPO ограничивает величину обновления политики с помощью функции потерь с обрезкой (clipping). Если обновление выходит за заранее заданные границы (обычно параметр от 0.1 до 0.3), оно "усекается", что предотвращает слишком большие шаги и обеспечивает стабильность обучения.

Оценка преимущества (advantage): Используется для определения, насколько действие было лучше среднего ожидания, что помогает усилить полезные действия и ослабить менее эффективные.

Преимущества PPO
Стабильность обучения: Благодаря механизму clipping PPO достигает устойчивого и плавного обучения без резких скачков в политике.

Относительная простота реализации: В сравнении с более сложными алгоритмами, PPO проще в реализации и настройке.

Хорошая производительность: PPO демонстрирует высокую эффективность на широком спектре задач, включая игры, робототехнику и задачи с непрерывным и дискретным пространством действий.

Недостатки PPO
Большое количество данных: Для достижения хороших результатов PPO может требовать значительных объёмов обучающих данных.

Требовательность к настройке: Несмотря на простоту, подбор гиперпараметров может потребовать времени и опыта.""",
    "DDPG (Deep Deterministic Policy Gradient)" : """Deep Deterministic Policy Gradient (DDPG) - это алгоритм обучения с подкреплением, который сочетает в себе методы детерминированной политики (Deterministic Policy Gradient, DPG) и глубокого обучения с Q-обучением (Deep Q-Network, DQN). Он предназначен для задач с непрерывным пространством действий, где агент должен выбирать значения из непрерывного диапазона, что особенно актуально для управления роботами, автономного вождения и других задач с высокоразмерными и непрерывными действиями.

Основные компоненты и принципы работы DDPG
Архитектура актор-критик: DDPG использует две нейронные сети - актор (Actor), который параметризует детерминированную стратегию и предсказывает конкретное действие для каждого состояния, и критик (Critic), который оценивает Q-функцию - ожидаемую суммарную награду за выполнение действия в данном состоянии.

Детерминированная политика: В отличие от стохастических методов, DDPG обучает детерминированную политику, которая для каждого состояния выдает конкретное действие, а не распределение вероятностей.

Использование целевых сетей (target networks): Для повышения стабильности обучения DDPG применяет целевые сети актор и критик, параметры которых обновляются медленно с помощью экспоненциального скользящего среднего. Это сглаживает обновления и предотвращает расходимость обучения.

Обучение критика: Критик обучается минимизировать ошибку между текущей оценкой Q-функции и целевым значением, вычисляемым с использованием целевой сети и награды, полученной от среды.

Обучение актера: Актор обновляется с помощью градиентного спуска по параметрам политики, направленного на максимизацию оценки критика, то есть на выбор действий, которые приносят максимальную ожидаемую награду.

Replay Buffer (буфер повторного воспроизведения): Для повышения эффективности и стабилизации обучения используется память с опытом, из которой случайным образом выбираются батчи для обучения, что уменьшает корреляцию между последовательными примерами.

Преимущества DDPG
Поддержка непрерывных действий: DDPG хорошо подходит для задач, где действия не дискретны, а принимают значения из непрерывного диапазона.

Высокая эффективность: Благодаря использованию целевых сетей и replay buffer, алгоритм обучается стабильно и эффективно.

Масштабируемость: DDPG может работать с высокоразмерными пространствами состояний и действий, что делает его применимым в сложных реальных задачах.

Недостатки DDPG
Чувствительность к гиперпараметрам: Алгоритм требует тщательной настройки параметров обучения для стабильной работы.

Проблемы с исследованием: Детерминированная политика может недостаточно исследовать пространство действий, что иногда приводит к попаданию в локальные минимумы.

Сложность обучения: В сравнении с некоторыми другими алгоритмами, обучение DDPG может быть менее стабильным и требовать дополнительных методов улучшения (например, TD3).""",
    "SAC (Soft Actor-Critic)" : """Soft Actor-Critic (SAC) - это современный алгоритм обучения с подкреплением, относящийся к семейству методов актор-критик и основанный на принципах максимизации энтропии. Его основная цель - обучить агента не только максимизировать ожидаемую награду, но и одновременно максимизировать энтропию политики, что способствует более эффективному исследованию среды и устойчивому обучению.

Основные особенности SAC
Максимизация энтропии: SAC добавляет к функции вознаграждения энтропийный член, который поощряет агента выбирать более разнообразные действия. Это помогает избежать преждевременной сходимости к неоптимальным стратегиям и улучшает исследование среды.

Актор-критик архитектура: SAC использует две нейронные сети - актор, который генерирует стохастическую политику (распределение действий), и критик, оценивающий качество действий через Q-функцию.

Двойной критик (Double Q-learning): Для уменьшения переоценки Q-значений SAC применяет два критика, что повышает стабильность и точность оценки.

Обучение с использованием off-policy данных: SAC эффективно обучается на данных, собранных ранее, что позволяет использовать replay buffer и повышает эффективность обучения.

Преимущества SAC
Устойчивость и стабильность: Максимизация энтропии и двойной критик делают алгоритм более стабильным по сравнению с традиционными методами.

Хорошее исследование среды: Благодаря энтропийному члену агент исследует пространство действий шире, что снижает риск застревания в локальных оптимумах.

Применимость к непрерывным и дискретным действиям: SAC подходит для широкого спектра задач, включая сложные среды с непрерывным пространством действий.

Высокая эффективность: SAC показывает отличные результаты на многих бенчмарках и считается одним из ведущих алгоритмов глубокого обучения с подкреплением.""",
    "TD3 (Twin Delayed DDPG)" : """Twin Delayed Deep Deterministic Policy Gradient (TD3) - это усовершенствованный алгоритм обучения с подкреплением для задач с непрерывным пространством действий, основанный на алгоритме Deep Deterministic Policy Gradient (DDPG). TD3 был разработан для повышения стабильности и эффективности обучения агентов в сложных средах.

Основные особенности TD3
Двойной критик (Double Critic): TD3 использует две нейронные сети-критика для оценки функции ценности (Q-функции). При обновлении критиков выбирается минимальное из двух значений Q, что помогает уменьшить переоценку значений, характерную для многих алгоритмов с критиком, и повышает стабильность обучения.

Задержанное обновление актера (Delayed Policy Updates): В отличие от DDPG, где актор и критик обновляются одновременно, в TD3 обновление актор-сети происходит реже (например, в два раза реже), что снижает шум в градиентах и улучшает качество политики.

Добавление шума к действиям (Target Policy Smoothing): Для повышения устойчивости TD3 добавляет небольшое случайное гауссовское возмущение к действиям, предсказанным целевой политикой. Это сглаживает Q-функцию и помогает избежать переобучения на точных действиях, улучшая обобщающую способность модели.

Replay Buffer: TD3 использует буфер повторного воспроизведения для хранения опыта агента и обучения на случайных батчах, что уменьшает корреляцию данных и стабилизирует процесс обучения.

Преимущества TD3
Снижение переоценки Q-функции: Использование двойного критика эффективно уменьшает смещение в оценках, что является одной из основных проблем в алгоритмах с критиком.

Улучшенная стабильность и эффективность: Задержанные обновления актор-сети и добавление шума к действиям делают обучение более стабильным и качественным по сравнению с DDPG.

Поддержка непрерывных действий: TD3 хорошо подходит для задач с непрерывным пространством действий, таких как управление роботами, автономное вождение и др.

Практическая применимость: TD3 демонстрирует state-of-the-art результаты на многих стандартных тестах по обучению с подкреплением."""}

model_params = {

    "A2C (Advantage Actor Critic)" : {
        'total_timesteps' : {"label": "total_timesteps", "type": "number_input", "min_value": 5000, "max_value": 1000000, "value": 5000, "step": 5000, 'help':'Определяет общее количество шагов среды (environment steps), на которых будет обучаться агент.'},
        'learning_rate' : {"label": "learning rate", "type": "number_input", "min_value": 0.0001, "max_value": 1.0, "value": 0.002, "step": 0.0001, "format": "%.4f", 'help':'Настраиваемый параметр алгоритма оптимизации, который определяет размер шага на каждой итерации, при движении к минимуму функции потерь.'},
        'ent_coef' :      {"label": "ent_coef", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.01, "step": 0.01, "format": "%.2f", 'help':'Коэффициент энтропии, который используется при вычислении функции потерь. Он регулирует влияние энтропии на общий лосс, стимулируя исследование (exploration) агентом.'},
        'gae_lambda':     {"label": "gae_lambda", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.95, "step": 0.01, "format": "%.2f", 'help':'Коэффициент, позволяющий найти компромисс между погрешностью и дисперсией для обобщенной оценки преимущества. Эквивалентно классическому преимуществу при значении 1.'},
        'gamma':          {"label": "gamma", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.98, "step": 0.01, "format": "%.2f", 'help':'Коэффициент дисконтирования будущих наград. Он определяет, насколько сильно агент учитывает будущие вознаграждения по сравнению с текущими.'},
        'max_grad_norm':  {"label": "max_grad_norm", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.5, "step": 0.01, "format": "%.2f", 'help':'Максимальное значение для обрезки градиента.'},
        'n_steps':  {"label": "n_steps", "type": "slider", "min_value": 1, "max_value": 10, "value": 8, "step": 1, 'help':'Количество шагов, выполняемых для каждой среды при каждом обновлении (т.е. размер пакета равен n_steps * n_env, где n_env - количество копий среды, выполняемых параллельно'},
        'normalize_advantage':  {"label": "normalize_advantage", "type": "selectbox", 'options' :[True, False], 'help':'Следует ли нормализовать или нет преимущество.'},
        'alpha':  {"label": "alpha",  "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.7, "step": 0.01, "format": "%.2f", 'help':'Параметр, который регулирует скорость или степень обновления модели в процессе обучения, влияя на стабильность и скорость сходимости алгоритма.'},
        'eps' : {"label": "eps", "type": "number_input", "min_value": 1e-06, "max_value":1e-02, "value": 1e-04, "step": 0.0001, "format": "%.4f", 'help':'Маленькое число для численной стабильности, например, добавляется к вероятностям или значениям для предотвращения ошибок вычислений.'},
    },

    "PPO (Proximal Policy Optimization)" : {
        'total_timesteps' : {"label": "total_timesteps", "type": "number_input", "min_value": 5000, "max_value": 1000000, "value": 50000, "step": 5000, 'help':'Определяет общее количество шагов среды (environment steps), на которых будет обучаться агент.'},
        'learning_rate' : {"label": "learning rate", "type": "number_input", "min_value": 0.0001, "max_value": 1.0, "value": 0.005, "step": 0.0001, "format": "%.4f", 'help':'Настраиваемый параметр алгоритма оптимизации, который определяет размер шага на каждой итерации, при движении к минимуму функции потерь.'},
        'batch_size' :    {"label": "batch_size", "type": "slider", "min_value": 16, "max_value": 512, "value": 64, "step": 2, 'help':'Размер мини-батча определяет сколько примеров опыта используется за один шаг оптимизации. Меньшие значения ускоряют обучение, но могут снизить стабильность.'},
        'ent_coef' :      {"label": "ent_coef", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.01, "step": 0.01, "format": "%.2f", 'help':'Коэффициент энтропии, который используется при вычислении функции потерь. Он регулирует влияние энтропии на общий лосс, стимулируя исследование (exploration) агентом.'},
        'vf_coef' :      {"label": "vf_coef", "type": "slider", "min_value": 0.1, "max_value": 1.0, "value": 0.5, "step": 0.05, "format": "%.2f", 'help':'Коэффициент функции ценности, балансирует вклад функции ценности в общую функцию потерь. Увеличение значения усиливает влияние ошибок предсказания ценности состояния.'},
        'gae_lambda':     {"label": "gae_lambda", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.95, "step": 0.01, "format": "%.2f", 'help':'Коэффициент, позволяющий найти компромисс между погрешностью и дисперсией для обобщенной оценки преимущества. Эквивалентно классическому преимуществу при значении 1.'},
        'gamma':          {"label": "gamma", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.98, "step": 0.01, "format": "%.2f", 'help':'Коэффициент дисконтирования будущих наград. Он определяет, насколько сильно агент учитывает будущие вознаграждения по сравнению с текущими.'},
        'max_grad_norm':  {"label": "max_grad_norm", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.5, "step": 0.01, "format": "%.2f", 'help':'Максимальное значение для обрезки градиента.'},
        'n_steps':  {"label": "n_steps", "type": "slider", "min_value": 1, "max_value": 10, "value": 3, "step": 1, 'help':'Количество шагов, выполняемых для каждой среды при каждом обновлении (т.е. размер пакета равен n_steps * n_env, где n_env - количество копий среды, выполняемых параллельно'},
        'normalize_advantage':  {"label": "normalize_advantage", "type": "selectbox", 'options' :[True, False], 'help':'Следует ли нормализовать или нет преимущество.'},

    },

    "DDPG (Deep Deterministic Policy Gradient)" : {
        'total_timesteps' : {"label": "total_timesteps", "type": "number_input", "min_value": 5000, "max_value": 1000000, "value": 50000, "step": 5000, 'help':'Определяет общее количество шагов среды (environment steps), на которых будет обучаться агент.'},
        'learning_rate' : {"label": "learning rate", "type": "number_input", "min_value": 0.0001, "max_value": 1.0, "value": 0.0050, "step": 0.0001, "format": "%.4f", 'help':'Настраиваемый параметр алгоритма оптимизации, который определяет размер шага на каждой итерации, при движении к минимуму функции потерь.'},
        'batch_size' :    {"label": "batch_size", "type": "slider", "min_value": 16, "max_value": 512, "value": 64, "step": 2, 'help':'Размер мини-батча определяет сколько примеров опыта используется за один шаг оптимизации. Меньшие значения ускоряют обучение, но могут снизить стабильность.'},
        'buffer_size' : {"label": "buffer_size", "type": "number_input", "min_value": 10000, "max_value": 200000, "value": 50000, "step": 10000, 'help':'Означает размер буфера воспроизведения (replay buffer) - количество сохранённых переходов (опыта) агента, которые могут использоваться для обучения.'},
        'gamma':          {"label": "gamma", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.98, "step": 0.01, "format": "%.2f", 'help':'Коэффициент дисконтирования будущих наград. Он определяет, насколько сильно агент учитывает будущие вознаграждения по сравнению с текущими.'},
        'gradient_steps':          {"label": "gradient_steps", "type": "slider", "min_value": -1, "max_value": 10, "value": 2, "step": 1, 'help':'Контролирует, сколько раз модель обновляется на основе накопленного опыта из replay buffer после каждого взаимодействия с окружением.'},
        'learning_starts' : {"label": "learning_starts", "type": "number_input", "min_value": 100.0, "max_value": 1000.0, "value": 500.0, "step": 50.0, "format": "%.1f", 'help':'Определяет, сколько шагов взаимодействия с окружением агент должен накопить в буфере воспроизведения (replay buffer) до начала обучения.'},
        'tau':          {"label": "tau", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.05, "step": 0.01, "format": "%.2f", 'help':'Коэффициент «мягкого» обновления целевых сетей актёра и критика.'},
    },

        "SAC (Soft Actor-Critic)" : {
        'total_timesteps' : {"label": "total_timesteps", "type": "number_input", "min_value": 5000, "max_value": 1000000, "value": 50000, "step": 5000, 'help':'Определяет общее количество шагов среды (environment steps), на которых будет обучаться агент.'},
        'learning_rate' : {"label": "learning rate", "type": "number_input", "min_value": 0.0001, "max_value": 1.0, "value": 0.0050, "step": 0.0001, "format": "%.4f", 'help':'Настраиваемый параметр алгоритма оптимизации, который определяет размер шага на каждой итерации, при движении к минимуму функции потерь.'},
        'batch_size' :    {"label": "batch_size", "type": "slider", "min_value": 16, "max_value": 512, "value": 64, "step": 2,  'help':'Размер мини-батча определяет сколько примеров опыта используется за один шаг оптимизации. Меньшие значения ускоряют обучение, но могут снизить стабильность.'},
        'buffer_size' : {"label": "buffer_size", "type": "number_input", "min_value": 10000, "max_value": 200000, "value": 50000, "step": 10000, 'help':'Означает размер буфера воспроизведения (replay buffer) - количество сохранённых переходов (опыта) агента, которые могут использоваться для обучения.'},
        'learning_starts' : {"label": "learning_starts", "type": "number_input", "min_value": 100.0, "max_value": 1000.0, "value": 500.0, "step": 50.0, "format": "%.1f", 'help':'Определяет, сколько шагов взаимодействия с окружением агент должен накопить в буфере воспроизведения (replay buffer) до начала обучения.'},
        'gamma':          {"label": "gamma", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.98, "step": 0.01, "format": "%.2f", 'help':'Коэффициент дисконтирования будущих наград. Он определяет, насколько сильно агент учитывает будущие вознаграждения по сравнению с текущими.'},
        'gradient_steps':          {"label": "gradient_steps", "type": "slider", "min_value": -1, "max_value": 10, "value": 2, "step": 1, 'help':'Контролирует, сколько раз модель обновляется на основе накопленного опыта из replay buffer после каждого взаимодействия с окружением.'},
        'tau':          {"label": "tau", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.05, "step": 0.01, "format": "%.2f", 'help':'Коэффициент «мягкого» обновления целевых сетей актёра и критика.'},
        'target_entropy':          {"label": "target_entropy", "type": "slider", "min_value": -50.0, "max_value": -1.0, "value": -10.0, "step": 1.0, "format": "%.1f", 'help':'Задаёт целевое значение энтропии политики, к которому алгоритм стремится во время обучения. Обычно это отрицательное число, примерно равное минусу размерности пространства действий.'}

    },

    "TD3 (Twin Delayed DDPG)" : {
        'total_timesteps' : {"label": "total_timesteps", "type": "number_input", "min_value": 5000, "max_value": 1000000, "value": 50000, "step": 5000, 'help':'Определяет общее количество шагов среды (environment steps), на которых будет обучаться агент.'},
        'learning_rate' : {"label": "learning rate", "type": "number_input", "min_value": 0.0001, "max_value": 1.0, "value": 0.0001, "step": 0.0001, "format": "%.4f", 'help':'Настраиваемый параметр алгоритма оптимизации, который определяет размер шага на каждой итерации, при движении к минимуму функции потерь.'},
        'batch_size' :    {"label": "batch_size", "type": "slider", "min_value": 16, "max_value": 512, "value": 128, "step": 2, 'help':'Размер мини-батча определяет сколько примеров опыта используется за один шаг оптимизации. Меньшие значения ускоряют обучение, но могут снизить стабильность.'},
        'learning_starts' : {"label": "learning_starts", "type": "number_input", "min_value": 100.0, "max_value": 1000.0, "value": 500.0, "step": 50.0, "format": "%.1f", 'help':'Определяет, сколько шагов взаимодействия с окружением агент должен накопить в буфере воспроизведения (replay buffer) до начала обучения.'},
        'buffer_size' : {"label": "buffer_size", "type": "number_input", "min_value": 10000, "max_value": 200000, "value": 50000, "step": 10000, 'help':'Означает размер буфера воспроизведения (replay buffer) - количество сохранённых переходов (опыта) агента, которые могут использоваться для обучения.'},
        'train_freq':          {"label": "train_freq", "type": "slider", "min_value": 1, "max_value": 100, "value": 10, "step": 1, 'help':'Определяет, как часто вызывается функция обучения (train) и выполняются градиентные шаги.'},
        'gamma':          {"label": "gamma", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.98, "step": 0.01, "format": "%.2f", 'help':'Коэффициент дисконтирования будущих наград. Он определяет, насколько сильно агент учитывает будущие вознаграждения по сравнению с текущими.'},
        'tau':          {"label": "tau", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.05, "step": 0.01, "format": "%.2f", 'help':'Коэффициент «мягкого» обновления целевых сетей актёра и критика.'},
    }


    #  "param1": {"label": "Learning Rate", "type": "number_input", "min_value": 0.0001, "max_value": 1.0, "value": 0.001, "step": 0.0001, "format": "%.4f"}
# "param2": {"label": "Batch Size", "type": "slider", "min_value": 16, "max_value": 512, "value": 128, "step": 16},
#         "param3": {"label": "Optimizer", "type": "selectbox", "options": ["Adam", "RMSprop", "SGD"]}


}

tooltip_text = """
macd (Moving Average Convergence Divergence) - индикатор, показывающий соотношение двух экспоненциальных скользящих средних (обычно 12 и 26 периодов).
boll_ub и boll_lb (верхняя и нижняя полосы Боллинджера) - границы, построенные на основе скользящей средней и стандартного отклонения цены.
rsi_30 (Relative Strength Index с периодом 30) - осциллятор, измеряющий скорость и изменение ценовых движений за 30 периодов.
cci_30 (Commodity Channel Index с периодом 30) - индикатор, измеряющий отклонение цены от её среднего значения за 30 периодов.
dx_30 (Directional Movement Index с периодом 30) - индекс направленного движения, оценивающий силу тренда за 30 периодов и показывающий, доминирует ли бычий или медвежий тренд.
close_30_sma и close_60_sma - простые скользящие средние цены закрытия за 30 и 60 периодов соответственно.
"""

sector_info = {
'AFLT': 'Транспорт',
    'AKRN': 'Химия',
    'ALRS': 'Металлургия',
    'BANE': 'Финансы',
    'BSPB': 'Финансы',
    'CBOM': 'Финансы',
    'CHMF': 'Металлургия',
    'ENPG': 'Энергетика',
    'FEES': 'Энергетика',
    'FESH': 'Транспорт',
    'FLOT': 'Транспорт',
    'GAZP': 'Нефтегаз',
    'GCHE': 'Розница',
    'GMKN': 'Металлургия',
    'HEAD': 'Услуги',
    'HYDR': 'Энергетика',
    'IRAO': 'Энергетика',
    'IRKT': 'Промышленность',
    'KZOS': 'Промышленность',
    'LKOH': 'Нефтегаз',
    'MAGN': 'Розница',
    'MGNT': 'Розница',
    'MOEX': 'Финансы',
    'MTSS': 'Телеком',
    'NKNC': 'Металлургия',
    'NLMK': 'Металлургия',
    'NMTP': 'Транспорт',
    'NVTK': 'Нефтегаз',
    'PHOR': 'Химия',
    'PIKK': 'Розница',
    'PLZL': 'Металлургия',
    'POSI': 'Финансы',
    'RASP': 'Транспорт',
    'ROSN': 'Нефтегаз',
    'RTKM': 'Телеком',
    'RUAL': 'Металлургия',
    'SBER': 'Финансы',
    'SBERP': 'Финансы',
    'SFIN': 'Финансы',
    'SIBN': 'Нефтегаз',
    'SNGS': 'Нефтегаз',
    'SNGSP': 'Нефтегаз',
    'SVCB': 'Финансы',
    'T': 'Финансы',
    'TATN': 'Нефтегаз',
    'UGLD': 'Химия',
    'UNAC': 'Энергетика',
    'UWGN': 'Промышленность',
    'VSMO': 'Медицина',
    'VTBR': 'Финансы',
    'YDEX': 'Технологии'
}
