stock_info = {
    'NVTK':[
        """
ПАО «НОВАТЭК» — крупнейший российский независимый производитель природного газа. Компания занимается разведкой, добычей, переработкой и реализацией природного газа и жидких углеводородов.
Основное стратегическое направление «НОВАТЭКа» — развитие конкурентоспособных мощностей для производства сжиженного природного газа (СПГ) и его экспорта на ведущие рынки. Также компания стремится увеличить добычу в высокоэффективных месторождениях с учётом принципов устойчивого развития.
        """
    ],
    'FESH' :
        [
            """
ПАО «Дальневосточное морское пароходство» (ДВМП) — головная компания транспортной группы FESCO, которая находится под управлением госкорпорации «Росатом».
Основная деятельность сосредоточена на Дальнем Востоке России. Компания осуществляет как внутрироссийские, так и международные контейнерные перевозки различными видами транспорта: морским, железнодорожным и автомобильным. FESCO организует регулярные контейнерные перевозки грузов из СНГ, стран Азии и Ближнего Востока через порты России и сухопутные пограничные переходы.
            """
        ],
    'LKOH':
    [
        """
«Лукойл» — одна из крупнейших вертикально интегрированных нефтегазовых компаний в мире. На долю компании приходится более 2% мировой добычи нефти и около 1% доказанных запасов углеводородов.
Основные направления деятельности:
Добыча и переработка нефти. « Лукойл» добывает нефть и газ по всему миру. Основные проекты компании находятся в России, Средней Азии и на Ближнем Востоке.
Сбыт нефти. В состав «Лукойла» входят четыре нефтеперерабатывающих завода: в Перми, Волгограде, Нижнем Новгороде и Ухте. Также у фирмы есть два НПЗ в Европе. Свою продукцию компания реализует в 19 странах мира.
        """
    ],
    'SBER':
    ["""«Сбербанк России» (полное наименование — Публичное акционерное общество «Сбербанк России») — российский финансовый конгломерат, крупнейший универсальный банк России и Восточной Европы.
Банк основан 22 марта 1991 года. К концу 2024 года у Сбербанка было 109,9 млн активных частных клиентов и 3,3 млн активных корпоративных клиентов."""],
'GAZP':["""Публичное акционерное общество «Газпром» (ПАО «Газпром») — российская транснациональная энергетическая компания. Более 50% (контрольный пакет) акций которой принадлежит государству.
Основные направления деятельности — геологоразведка, добыча, транспортировка, хранение, переработка и реализация газа, газового конденсата и нефти, реализация газа в качестве моторного топлива, а также производство и сбыт тепло- и электроэнергии.
Компания владеет крупнейшей в мире газотранспортной системой, протяжённость которой составляет 180,6 тыс. км на территории РФ. На «Газпром» приходится 12% мировой и 68% российской добычи газа."""],
'NMTP':["""ПАО «Новороссийский морской торговый порт» (НМТП) — российская компания, крупнейший портовый оператор, осуществляющая свою деятельность в Новороссийском морском порту. Полное наименование — Публичное акционерное общество «Новороссийский морской торговый порт».
Группа НМТП является третьим портовым оператором Европы и безусловным лидером на российском стивидорном рынке по объёму грузооборота. В состав Группы входят два крупнейших порта России — порт Новороссийск на Чёрном море, порт Приморск на Балтийском море, а также порт Балтийск в Калининградской области."""],
'T':["""«Т-Банк» (до 5 июня 2024 года — «Тинькофф Банк») — российский коммерческий банк, сфокусированный полностью на дистанционном обслуживании, не имеющий розничных отделений.
Основан в 2006 году предпринимателем Олегом Тиньковым (объявлен иноагентом в РФ). К 2024 году стал одним из крупнейших банков России, который обслуживает миллионы клиентов и предлагает широкий спектр финансовых услуг."""],
'HEAD':["""HeadHunter — платформа для поиска работы и подбора персонала в Москве и по всей России. Сайт предлагает обширную базу вакансий и резюме, а также ряд инструментов для эффективного поиска.
Миссия платформы заключается в том, чтобы помогать людям находить работу, а компаниям — наилучших сотрудников."""],
'ROSN':["""Роснефть — лидер российской нефтяной отрасли и одна из крупнейших публичных нефтегазовых компаний мира.Основные виды деятельности: разведка и добыча нефти и газа, производство нефтепродуктов и продукции нефтехимии, а также сбыт произведённой продукции.
География деятельности в секторе разведки и добычи охватывает все основные нефтегазоносные провинции России: Западную Сибирь, Южную и Центральную Россию, Тимано-Печору, Восточную Сибирь и Дальний Восток. Компания также реализует проекты в Казахстане, Алжире и Туркменистане."""],
'MGNT':["""Российская сеть розничных магазинов. Основана в 1994 году в Краснодаре Сергеем Галицким, владевшим и управлявшим компанией до 2018 года. С 2021 года крупнейший акционер - инвестиционная компания Marathon Group Александра Винокурова."""],
'NLMK':["""Группа НЛМК — крупнейший производитель стали в России, входит в топ-20 глобальных металлургических компаний. Использует вертикально интегрированную бизнес-модель — от добычи сырья и производства стали до производства готовой продукции, сервисного обслуживания и дистрибуции.
Располагает 17 производственными площадками в России, Европе, США и Индии. Металлопродукция Группы НЛМК используется в различных отраслях, от строительства, автомобильной промышленности и машиностроения до производства оффшорных ветровых установок, грузовых кораблей и многих других."""],
'AFLT':["""«Аэрофлот» — крупнейшая авиакомпания России с сетью дочерних предприятий. Выполняет внутренние и международные перевозки, располагает широкой маршрутной сетью и самым крупным воздушным флотом в СНГ."""],
'MTSS':["""Российская компания, предоставляющая телекоммуникационные услуги, цифровые и медийные сервисы в России и Белоруссии под торговой маркой «МТС». Крупнейший по количеству абонентов оператор связи в России. Компания оказывает услуги сотовой связи. По состоянию на сентябрь 2024 года компания обслужила 87,6 миллионов абонентов, в России - 81,9 млн., в Белоруссии - 5,7 млн.. Ранее МТС работала в Узбекистане до 2012 года, в Туркменистане до 2017 года, на Украине до 2019 года и в Армении до 2024 года."""],
'MOEX':["""Московская биржа — единственная в России многофункциональная биржевая площадка по торговле валютой, акциями, облигациями, производными инструментами, инструментами денежного рынка и товарами.
Торги на Мосбирже идут ежедневно, за исключением выходных и государственных праздников. Для каждого рынка устанавливается свой временной диапазон, в который можно совершать сделки."""]
}

model_info = {
    "A2C (Advantage Actor Critic)" : """
Advantage Actor Critic (A2C) - это алгоритм обучения с подкреплением, который объединяет преимущества двух подходов: обучения на основе политики (policy-based) и обучения на основе значения (value-based). В основе A2C лежит архитектура Actor-Critic, где два компонента работают совместно: актор (Actor), который отвечает за выбор действий, и критик (Critic), который оценивает качество этих действий.

Как работает A2C:

Актор формирует политику - распределение вероятностей действий в каждом состоянии, то есть решает, какое действие выполнить.

Критик оценивает, насколько хорошим было выбранное действие, вычисляя функцию преимущества (Advantage function). Эта функция показывает, насколько действие лучше среднего ожидания для данного состояния.

Функция преимущества вычисляется как разница между оценкой качества конкретного действия и средней ценностью состояния. Если значение преимущества положительно, это означает, что действие было лучше среднего, и актору следует увеличить вероятность его выбора. Если отрицательно - вероятность действия уменьшается.

Преимущества A2C:

Стабилизация обучения: Использование функции преимущества снижает дисперсию градиентов по сравнению с классическими методами Policy Gradient, что ускоряет и стабилизирует процесс обучения.

Синхронность: В отличие от асинхронного A3C, A2C синхронно обновляет параметры, что позволяет эффективнее использовать вычислительные ресурсы, особенно GPU, за счёт больших батчей данных.

Обновление на каждом шаге: Критик обновляется после каждого шага, а не по окончании эпизода, что повышает скорость и качество обучения.

Гибкость: A2C хорошо подходит для задач с непрерывным и дискретным пространством действий, широко применяется в играх и робототехнике.
""",
    "PPO (Proximal Policy Optimization)" : """Proximal Policy Optimization (PPO) - это современный алгоритм обучения с подкреплением, который сочетает в себе эффективность и стабильность при обучении агентов принимать решения в сложных средах. PPO относится к семейству методов оптимизации политики и использует архитектуру актор-критик, где актор отвечает за выбор действий, а критик оценивает качество этих действий.

Основные принципы работы PPO
Оптимизация политики через градиентный спуск: PPO стремится максимизировать ожидаемую награду агента, обновляя параметры политики с помощью градиентного метода.

Контроль изменений политики: Чтобы избежать резких и дестабилизирующих изменений стратегии агента, PPO вводит отношение вероятностей выбора действий новой и старой политиками. Это отношение показывает, насколько сильно обновилась политика.

Механизм clipping: PPO ограничивает величину обновления политики с помощью функции потерь с обрезкой (clipping). Если обновление выходит за заранее заданные границы (обычно параметр от 0.1 до 0.3), оно "усекается", что предотвращает слишком большие шаги и обеспечивает стабильность обучения.

Оценка преимущества (advantage): Используется для определения, насколько действие было лучше среднего ожидания, что помогает усилить полезные действия и ослабить менее эффективные.

Преимущества PPO
Стабильность обучения: Благодаря механизму clipping PPO достигает устойчивого и плавного обучения без резких скачков в политике.

Относительная простота реализации: В сравнении с более сложными алгоритмами, PPO проще в реализации и настройке.

Хорошая производительность: PPO демонстрирует высокую эффективность на широком спектре задач, включая игры, робототехнику и задачи с непрерывным и дискретным пространством действий.

Недостатки PPO
Большое количество данных: Для достижения хороших результатов PPO может требовать значительных объёмов обучающих данных.

Требовательность к настройке: Несмотря на простоту, подбор гиперпараметров может потребовать времени и опыта.""",
    "DDPG (Deep Deterministic Policy Gradient)" : """Deep Deterministic Policy Gradient (DDPG) - это алгоритм обучения с подкреплением, который сочетает в себе методы детерминированной политики (Deterministic Policy Gradient, DPG) и глубокого обучения с Q-обучением (Deep Q-Network, DQN). Он предназначен для задач с непрерывным пространством действий, где агент должен выбирать значения из непрерывного диапазона, что особенно актуально для управления роботами, автономного вождения и других задач с высокоразмерными и непрерывными действиями.

Основные компоненты и принципы работы DDPG
Архитектура актор-критик: DDPG использует две нейронные сети - актор (Actor), который параметризует детерминированную стратегию и предсказывает конкретное действие для каждого состояния, и критик (Critic), который оценивает Q-функцию - ожидаемую суммарную награду за выполнение действия в данном состоянии.

Детерминированная политика: В отличие от стохастических методов, DDPG обучает детерминированную политику, которая для каждого состояния выдает конкретное действие, а не распределение вероятностей.

Использование целевых сетей (target networks): Для повышения стабильности обучения DDPG применяет целевые сети актор и критик, параметры которых обновляются медленно с помощью экспоненциального скользящего среднего. Это сглаживает обновления и предотвращает расходимость обучения.

Обучение критика: Критик обучается минимизировать ошибку между текущей оценкой Q-функции и целевым значением, вычисляемым с использованием целевой сети и награды, полученной от среды.

Обучение актера: Актор обновляется с помощью градиентного спуска по параметрам политики, направленного на максимизацию оценки критика, то есть на выбор действий, которые приносят максимальную ожидаемую награду.

Replay Buffer (буфер повторного воспроизведения): Для повышения эффективности и стабилизации обучения используется память с опытом, из которой случайным образом выбираются батчи для обучения, что уменьшает корреляцию между последовательными примерами.

Преимущества DDPG
Поддержка непрерывных действий: DDPG хорошо подходит для задач, где действия не дискретны, а принимают значения из непрерывного диапазона.

Высокая эффективность: Благодаря использованию целевых сетей и replay buffer, алгоритм обучается стабильно и эффективно.

Масштабируемость: DDPG может работать с высокоразмерными пространствами состояний и действий, что делает его применимым в сложных реальных задачах.

Недостатки DDPG
Чувствительность к гиперпараметрам: Алгоритм требует тщательной настройки параметров обучения для стабильной работы.

Проблемы с исследованием: Детерминированная политика может недостаточно исследовать пространство действий, что иногда приводит к попаданию в локальные минимумы.

Сложность обучения: В сравнении с некоторыми другими алгоритмами, обучение DDPG может быть менее стабильным и требовать дополнительных методов улучшения (например, TD3).""",
    "SAC (Soft Actor-Critic)" : """Soft Actor-Critic (SAC) - это современный алгоритм обучения с подкреплением, относящийся к семейству методов актор-критик и основанный на принципах максимизации энтропии. Его основная цель - обучить агента не только максимизировать ожидаемую награду, но и одновременно максимизировать энтропию политики, что способствует более эффективному исследованию среды и устойчивому обучению.

Основные особенности SAC
Максимизация энтропии: SAC добавляет к функции вознаграждения энтропийный член, который поощряет агента выбирать более разнообразные действия. Это помогает избежать преждевременной сходимости к неоптимальным стратегиям и улучшает исследование среды.

Актор-критик архитектура: SAC использует две нейронные сети - актор, который генерирует стохастическую политику (распределение действий), и критик, оценивающий качество действий через Q-функцию.

Двойной критик (Double Q-learning): Для уменьшения переоценки Q-значений SAC применяет два критика, что повышает стабильность и точность оценки.

Обучение с использованием off-policy данных: SAC эффективно обучается на данных, собранных ранее, что позволяет использовать replay buffer и повышает эффективность обучения.

Преимущества SAC
Устойчивость и стабильность: Максимизация энтропии и двойной критик делают алгоритм более стабильным по сравнению с традиционными методами.

Хорошее исследование среды: Благодаря энтропийному члену агент исследует пространство действий шире, что снижает риск застревания в локальных оптимумах.

Применимость к непрерывным и дискретным действиям: SAC подходит для широкого спектра задач, включая сложные среды с непрерывным пространством действий.

Высокая эффективность: SAC показывает отличные результаты на многих бенчмарках и считается одним из ведущих алгоритмов глубокого обучения с подкреплением.""",
    "TD3 (Twin Delayed DDPG)" : """Twin Delayed Deep Deterministic Policy Gradient (TD3) - это усовершенствованный алгоритм обучения с подкреплением для задач с непрерывным пространством действий, основанный на алгоритме Deep Deterministic Policy Gradient (DDPG). TD3 был разработан для повышения стабильности и эффективности обучения агентов в сложных средах.

Основные особенности TD3
Двойной критик (Double Critic): TD3 использует две нейронные сети-критика для оценки функции ценности (Q-функции). При обновлении критиков выбирается минимальное из двух значений Q, что помогает уменьшить переоценку значений, характерную для многих алгоритмов с критиком, и повышает стабильность обучения.

Задержанное обновление актера (Delayed Policy Updates): В отличие от DDPG, где актор и критик обновляются одновременно, в TD3 обновление актор-сети происходит реже (например, в два раза реже), что снижает шум в градиентах и улучшает качество политики.

Добавление шума к действиям (Target Policy Smoothing): Для повышения устойчивости TD3 добавляет небольшое случайное гауссовское возмущение к действиям, предсказанным целевой политикой. Это сглаживает Q-функцию и помогает избежать переобучения на точных действиях, улучшая обобщающую способность модели.

Replay Buffer: TD3 использует буфер повторного воспроизведения для хранения опыта агента и обучения на случайных батчах, что уменьшает корреляцию данных и стабилизирует процесс обучения.

Преимущества TD3
Снижение переоценки Q-функции: Использование двойного критика эффективно уменьшает смещение в оценках, что является одной из основных проблем в алгоритмах с критиком.

Улучшенная стабильность и эффективность: Задержанные обновления актор-сети и добавление шума к действиям делают обучение более стабильным и качественным по сравнению с DDPG.

Поддержка непрерывных действий: TD3 хорошо подходит для задач с непрерывным пространством действий, таких как управление роботами, автономное вождение и др.

Практическая применимость: TD3 демонстрирует state-of-the-art результаты на многих стандартных тестах по обучению с подкреплением."""}

model_params = {

    "A2C (Advantage Actor Critic)" : {
        'total_timesteps' : {"label": "total_timesteps", "type": "number_input", "min_value": 5000, "max_value": 1000000, "value": 5000, "step": 10000, 'help':'Определяет общее количество шагов среды (environment steps), на которых будет обучаться агент.'},
        'learning_rate' : {"label": "learning rate", "type": "number_input", "min_value": 0.0001, "max_value": 1.0, "value": 0.005, "step": 0.0001, "format": "%.4f", 'help':'Настраиваемый параметр алгоритма оптимизации, который определяет размер шага на каждой итерации, при движении к минимуму функции потерь.'},
        'ent_coef' :      {"label": "ent_coef", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.01, "step": 0.01, "format": "%.2f", 'help':'Коэффициент энтропии, который используется при вычислении функции потерь. Он регулирует влияние энтропии на общий лосс, стимулируя исследование (exploration) агентом.'},
        'gae_lambda':     {"label": "gae_lambda", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.95, "step": 0.01, "format": "%.2f", 'help':'Коэффициент, позволяющий найти компромисс между погрешностью и дисперсией для обобщенной оценки преимущества. Эквивалентно классическому преимуществу при значении 1.'},
        'gamma':          {"label": "gamma", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.98, "step": 0.01, "format": "%.2f", 'help':'Коэффициент дисконтирования будущих наград. Он определяет, насколько сильно агент учитывает будущие вознаграждения по сравнению с текущими.'},
        'max_grad_norm':  {"label": "max_grad_norm", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.5, "step": 0.01, "format": "%.2f", 'help':'Максимальное значение для обрезки градиента.'},
        'n_steps':  {"label": "n_steps", "type": "slider", "min_value": 1, "max_value": 10, "value": 3, "step": 1, 'help':'Количество шагов, выполняемых для каждой среды при каждом обновлении (т.е. размер пакета равен n_steps * n_env, где n_env - количество копий среды, выполняемых параллельно'},
        'normalize_advantage':  {"label": "normalize_advantage", "type": "selectbox", 'options' :[True, False], 'help':'Следует ли нормализовать или нет преимущество.'},
        'alpha':  {"label": "alpha",  "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.7, "step": 0.01, "format": "%.2f", 'help':'Параметр, который регулирует скорость или степень обновления модели в процессе обучения, влияя на стабильность и скорость сходимости алгоритма.'},
        'eps' : {"label": "eps", "type": "number_input", "min_value": 1e-06, "max_value":1e-02, "value": 1e-04, "step": 0.0001, "format": "%.4f", 'help':'Маленькое число для численной стабильности, например, добавляется к вероятностям или значениям для предотвращения ошибок вычислений.'},
    },

    "PPO (Proximal Policy Optimization)" : {
        'total_timesteps' : {"label": "total_timesteps", "type": "number_input", "min_value": 20000, "max_value": 1000000, "value": 50000, "step": 10000, 'help':'Определяет общее количество шагов среды (environment steps), на которых будет обучаться агент.'},
        'learning_rate' : {"label": "learning rate", "type": "number_input", "min_value": 0.0001, "max_value": 1.0, "value": 0.005, "step": 0.0001, "format": "%.4f", 'help':'Настраиваемый параметр алгоритма оптимизации, который определяет размер шага на каждой итерации, при движении к минимуму функции потерь.'},
        'batch_size' :    {"label": "batch_size", "type": "slider", "min_value": 16, "max_value": 512, "value": 64, "step": 2, 'help':'Размер мини-батча определяет сколько примеров опыта используется за один шаг оптимизации. Меньшие значения ускоряют обучение, но могут снизить стабильность.'},
        'ent_coef' :      {"label": "ent_coef", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.01, "step": 0.01, "format": "%.2f", 'help':'Коэффициент энтропии, который используется при вычислении функции потерь. Он регулирует влияние энтропии на общий лосс, стимулируя исследование (exploration) агентом.'},
        'vf_coef' :      {"label": "vf_coef", "type": "slider", "min_value": 0.1, "max_value": 1.0, "value": 0.5, "step": 0.05, "format": "%.2f", 'help':'Коэффициент функции ценности, балансирует вклад функции ценности в общую функцию потерь. Увеличение значения усиливает влияние ошибок предсказания ценности состояния.'},
        'gae_lambda':     {"label": "gae_lambda", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.95, "step": 0.01, "format": "%.2f", 'help':'Коэффициент, позволяющий найти компромисс между погрешностью и дисперсией для обобщенной оценки преимущества. Эквивалентно классическому преимуществу при значении 1.'},
        'gamma':          {"label": "gamma", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.98, "step": 0.01, "format": "%.2f", 'help':'Коэффициент дисконтирования будущих наград. Он определяет, насколько сильно агент учитывает будущие вознаграждения по сравнению с текущими.'},
        'max_grad_norm':  {"label": "max_grad_norm", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.5, "step": 0.01, "format": "%.2f", 'help':'Максимальное значение для обрезки градиента.'},
        'n_steps':  {"label": "n_steps", "type": "slider", "min_value": 1, "max_value": 10, "value": 3, "step": 1, 'help':'Количество шагов, выполняемых для каждой среды при каждом обновлении (т.е. размер пакета равен n_steps * n_env, где n_env - количество копий среды, выполняемых параллельно'},
        'normalize_advantage':  {"label": "normalize_advantage", "type": "selectbox", 'options' :[True, False], 'help':'Следует ли нормализовать или нет преимущество.'},

    },

    "DDPG (Deep Deterministic Policy Gradient)" : {
        'total_timesteps' : {"label": "total_timesteps", "type": "number_input", "min_value": 20000, "max_value": 1000000, "value": 50000, "step": 10000, 'help':'Определяет общее количество шагов среды (environment steps), на которых будет обучаться агент.'},
        'learning_rate' : {"label": "learning rate", "type": "number_input", "min_value": 0.0001, "max_value": 1.0, "value": 0.0050, "step": 0.0001, "format": "%.4f", 'help':'Настраиваемый параметр алгоритма оптимизации, который определяет размер шага на каждой итерации, при движении к минимуму функции потерь.'},
        'batch_size' :    {"label": "batch_size", "type": "slider", "min_value": 16, "max_value": 512, "value": 64, "step": 2, 'help':'Размер мини-батча определяет сколько примеров опыта используется за один шаг оптимизации. Меньшие значения ускоряют обучение, но могут снизить стабильность.'},
        'buffer_size' : {"label": "buffer_size", "type": "number_input", "min_value": 10000, "max_value": 200000, "value": 50000, "step": 10000, 'help':'Означает размер буфера воспроизведения (replay buffer) - количество сохранённых переходов (опыта) агента, которые могут использоваться для обучения.'},
        'gamma':          {"label": "gamma", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.98, "step": 0.01, "format": "%.2f", 'help':'Коэффициент дисконтирования будущих наград. Он определяет, насколько сильно агент учитывает будущие вознаграждения по сравнению с текущими.'},
        'gradient_steps':          {"label": "gradient_steps", "type": "slider", "min_value": -1, "max_value": 10, "value": 2, "step": 1, 'help':'Контролирует, сколько раз модель обновляется на основе накопленного опыта из replay buffer после каждого взаимодействия с окружением.'},
        'learning_starts' : {"label": "learning_starts", "type": "number_input", "min_value": 100.0, "max_value": 1000.0, "value": 500.0, "step": 50.0, "format": "%.1f", 'help':'Определяет, сколько шагов взаимодействия с окружением агент должен накопить в буфере воспроизведения (replay buffer) до начала обучения.'},
        'tau':          {"label": "tau", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.05, "step": 0.01, "format": "%.2f", 'help':'Коэффициент «мягкого» обновления целевых сетей актёра и критика.'},
    },

        "SAC (Soft Actor-Critic)" : {
        'total_timesteps' : {"label": "total_timesteps", "type": "number_input", "min_value": 20000, "max_value": 1000000, "value": 50000, "step": 10000, 'help':'Определяет общее количество шагов среды (environment steps), на которых будет обучаться агент.'},
        'learning_rate' : {"label": "learning rate", "type": "number_input", "min_value": 0.0001, "max_value": 1.0, "value": 0.0050, "step": 0.0001, "format": "%.4f", 'help':'Настраиваемый параметр алгоритма оптимизации, который определяет размер шага на каждой итерации, при движении к минимуму функции потерь.'},
        'batch_size' :    {"label": "batch_size", "type": "slider", "min_value": 16, "max_value": 512, "value": 64, "step": 2,  'help':'Размер мини-батча определяет сколько примеров опыта используется за один шаг оптимизации. Меньшие значения ускоряют обучение, но могут снизить стабильность.'},
        'buffer_size' : {"label": "buffer_size", "type": "number_input", "min_value": 10000, "max_value": 200000, "value": 50000, "step": 10000, 'help':'Означает размер буфера воспроизведения (replay buffer) - количество сохранённых переходов (опыта) агента, которые могут использоваться для обучения.'},
        'learning_starts' : {"label": "learning_starts", "type": "number_input", "min_value": 100.0, "max_value": 1000.0, "value": 500.0, "step": 50.0, "format": "%.1f", 'help':'Определяет, сколько шагов взаимодействия с окружением агент должен накопить в буфере воспроизведения (replay buffer) до начала обучения.'},
        'gamma':          {"label": "gamma", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.98, "step": 0.01, "format": "%.2f", 'help':'Коэффициент дисконтирования будущих наград. Он определяет, насколько сильно агент учитывает будущие вознаграждения по сравнению с текущими.'},
        'gradient_steps':          {"label": "gradient_steps", "type": "slider", "min_value": -1, "max_value": 10, "value": 2, "step": 1, 'help':'Контролирует, сколько раз модель обновляется на основе накопленного опыта из replay buffer после каждого взаимодействия с окружением.'},
        'tau':          {"label": "tau", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.05, "step": 0.01, "format": "%.2f", 'help':'Коэффициент «мягкого» обновления целевых сетей актёра и критика.'},
        'target_entropy':          {"label": "target_entropy", "type": "slider", "min_value": -50.0, "max_value": -1.0, "value": -10.0, "step": 1.0, "format": "%.1f", 'help':'Задаёт целевое значение энтропии политики, к которому алгоритм стремится во время обучения. Обычно это отрицательное число, примерно равное минусу размерности пространства действий.'}

    },

    "TD3 (Twin Delayed DDPG)" : {
        'total_timesteps' : {"label": "total_timesteps", "type": "number_input", "min_value": 20000, "max_value": 1000000, "value": 50000, "step": 10000, 'help':'Определяет общее количество шагов среды (environment steps), на которых будет обучаться агент.'},
        'learning_rate' : {"label": "learning rate", "type": "number_input", "min_value": 0.0001, "max_value": 1.0, "value": 0.0001, "step": 0.0001, "format": "%.4f", 'help':'Настраиваемый параметр алгоритма оптимизации, который определяет размер шага на каждой итерации, при движении к минимуму функции потерь.'},
        'batch_size' :    {"label": "batch_size", "type": "slider", "min_value": 16, "max_value": 512, "value": 128, "step": 2, 'help':'Размер мини-батча определяет сколько примеров опыта используется за один шаг оптимизации. Меньшие значения ускоряют обучение, но могут снизить стабильность.'},
        'learning_starts' : {"label": "learning_starts", "type": "number_input", "min_value": 100.0, "max_value": 1000.0, "value": 500.0, "step": 50.0, "format": "%.1f", 'help':'Определяет, сколько шагов взаимодействия с окружением агент должен накопить в буфере воспроизведения (replay buffer) до начала обучения.'},
        'buffer_size' : {"label": "buffer_size", "type": "number_input", "min_value": 10000, "max_value": 200000, "value": 50000, "step": 10000, 'help':'Означает размер буфера воспроизведения (replay buffer) - количество сохранённых переходов (опыта) агента, которые могут использоваться для обучения.'},
        'train_freq':          {"label": "train_freq", "type": "slider", "min_value": 1, "max_value": 100, "value": 10, "step": 1, 'help':'Определяет, как часто вызывается функция обучения (train) и выполняются градиентные шаги.'},
        'gamma':          {"label": "gamma", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.98, "step": 0.01, "format": "%.2f", 'help':'Коэффициент дисконтирования будущих наград. Он определяет, насколько сильно агент учитывает будущие вознаграждения по сравнению с текущими.'},
        'tau':          {"label": "tau", "type": "slider", "min_value": 0.01, "max_value": 1.0, "value": 0.05, "step": 0.01, "format": "%.2f", 'help':'Коэффициент «мягкого» обновления целевых сетей актёра и критика.'},
    }


    #  "param1": {"label": "Learning Rate", "type": "number_input", "min_value": 0.0001, "max_value": 1.0, "value": 0.001, "step": 0.0001, "format": "%.4f"}
# "param2": {"label": "Batch Size", "type": "slider", "min_value": 16, "max_value": 512, "value": 128, "step": 16},
#         "param3": {"label": "Optimizer", "type": "selectbox", "options": ["Adam", "RMSprop", "SGD"]}


}

tooltip_text = """
macd (Moving Average Convergence Divergence) - индикатор, показывающий соотношение двух экспоненциальных скользящих средних (обычно 12 и 26 периодов). 
boll_ub и boll_lb (верхняя и нижняя полосы Боллинджера) - границы, построенные на основе скользящей средней и стандартного отклонения цены.
rsi_30 (Relative Strength Index с периодом 30) - осциллятор, измеряющий скорость и изменение ценовых движений за 30 периодов.
cci_30 (Commodity Channel Index с периодом 30) - индикатор, измеряющий отклонение цены от её среднего значения за 30 периодов.
dx_30 (Directional Movement Index с периодом 30) - индекс направленного движения, оценивающий силу тренда за 30 периодов и показывающий, доминирует ли бычий или медвежий тренд.
close_30_sma и close_60_sma - простые скользящие средние цены закрытия за 30 и 60 периодов соответственно.
"""